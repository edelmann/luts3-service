This document intends to gave an overview of the SGAS LUTS system.

The SGAS LUTS system is intended for accounting infrastructure in grid and HPC
systems.



1: Terminology & Concepts


* Usage Record

An XML document adhering the OGF-98 standard. The document describes the job
and consumed resource for a job run on a grid or cluster. Most accounting
systems use a number of extensions for the OGF standard to describe additional
information.

Usage Records is typically abbreviated to UR.


* LUTS service:

A service to which usage records can be registered to and stored at. Aggregated
information can then be extracted from the LUTS service through views.

Typically a single LUTS would be run per-country or per-vo basis, but it can
differ. It is also possible to run a LUTS per site, however this might incur a
lot of additional adminstration time. The options can also be combined in
various ways, e.g., having a per-site LUTS and a national LUTS. Note that the
LUTS does not feature forwarding of usage records. Instead the registration
client simply registers URs to multple places. This scheme is significantly
simpler and less brittle than forwarding.

Currently NDGF runs a single LUTS to which all ATLAS and ALICE production sites
report to. In the future it is planned/hoped that the NGIs will run national
LUTSes, and that only ATLAS, ALICE, CMS URs will get logged to the LUTS run by
NDGF.


* UR Logger:

A program/plugin generating usage records. Typically installed/deployed
alongside a grid middleware or LRMS.

While the usage record could be registered immediately it is typically written
to disk in a spool directory and then registered to one or more LUTSes by
another program, typically called UR registrant.

The ARC middleware includes a UR logger (and registrant) as of version 0.8.1.

NDGF is also running a custom UR logger for MonALISA (the information system of
the AliEn production system/grid).

NDGF is working on producing UR loggers for LRMSes. Currently Maui is underway
as the first target. Code is available at: http://github.com/htj/lrmsurgen

The three loggers all use the same registrant code (except some minor
differences). The current registrant supports registering URs to multiple
LUTSes, per-VO registration and will retry registration if the LUTS is
unavailable. This will also work with multiple LUTSes and so forth.



2. LUTS Service


* Description:

The LUTS service is daemon listening on port 6143 by default. The
interface/protocol is TLS+HTTP, and is used for registering URs and accessing
views (can be done via browser or view data can be fetched using ordinary HTTP
client libraries).

The LUTS service is essentially a thin wrapper over a database (the server code
is only ~1500 lines). It performs mainly authorization and data transformation.
Proxy certificates are not yet supported, but host certificates (for
registration), and ordinary grid user certificates (for accessing view in in the
browser), works fine.

The underlying database is CouchDB, a document-oriented JSON database. It has a
web page at: http://couchdb.apache.org/


* Insertion

Once the LUTS has received a batch of URs, the usage records are transformed
into JSON documents before being inserted into the database (which is JSON
based). Information about insert time, certificate used in authorization and
from which host the usage record was inserted is added as "provenance"
information the usage records.

An _id field (the reference for the document in the database) is added to the
JSON document. Currently this is a SHA224 hash of the global_job_id value. This
may change in the future, as it is rather inefficient for view creation.


* Views

Note: The view engine in SGAS is not yet deemed ready for general usage, but it
can still be very useful.

Just inserting usage records in itself is rather useless. In order to provide
high level information about the usage record data, SGAS provides access to
aggregated information in the form of views. It is also possible to access
individual usage records, but this is typically rather meaningless (as studying
a drop in an ocean).

SGAS tries to push as much view functionality to CouchDB, as the database is
closest to the data  Often SGAS will just pass the data directly though it, or
do some basic data transformation.

Creating an SGAS view consists of two steps: Creating a view in CouchDB, and
creating the view in SGAS. The first is the most tricky one. An introduction to
CouchDB view can found here:
http://wiki.apache.org/couchdb/Introduction_to_CouchDB_views

In the future we intend to deliver "standard" views with SGAS, for the most
common usage patterns. For now, they must be created manually.


* Creating Views

You will probably need an example UR to look at. One can be found in
docs/ur.json. Note that not all URs have all information.

1. Figure out what information should be extracted.

First establish what information is wanted in the view.

As an example we would like to know much accumulated walltime each host has
been used each day.


2. Write the map-reduce query.

CouchDB view are based on the map-reduce model. First each document in the
database is mapped to a set of key-value entries. The number of produced
entries can be 0 or multiple, but is typically one. The mapping function would
look like this:

function(doc) {
    emit([doc.machine_name, doc.end_time.substring(0,10)], doc.wall_duration);
}

This produces entries with (host, date) as key, and wall time (in seconds) as
the value. To add the seconds together, we define the reduce function as this:

function (key, values) {
    return sum(values);
}

This wall add the values for all identical keys together in order to produce a
single value for the host+date combination. The easiest way to create the view
in CouchDB is to create a temporary view and the save it as a permanent view.


3. Configure SGAS

Having created the view in CouchDB, SGAS needs to be aware of it. This is done
by creating an entry in sgas.conf, like this:

[view:hostwalltime]
design=host
view=walltime
description=Accumulated walltime per host / day

The first line creates a configuration entry, and names the view. With the
given name, the view would be accessed at:
https://sgas.examples.org:6143/sgas/view/hostwalltime

The design and view options define the location of the view in CouchDB. The
description adds a humanly readable description to the view. More options are
available for data filtering or post processing.


4. Authorize Users

Finally users must be allowed access to the view. This is done be adding a line
to sgas.authz, like this:

"/O=Grid/O=NorduGrid/OU=Organization/CN=Grid User"     view:hostwalltime

This line allows the user in question access to the view.


* More Information

This documentation is a work in progress.

If you have questions or suggestions, please contact htj _at_ ndgf.org

